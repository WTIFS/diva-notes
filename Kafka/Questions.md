##### Kafka吞吐量大的原因

- 磁盘顺序读写
- 分区分段 + 索引（分`topic`、`broker`、`segment`）
- 零拷贝（发送使用 `sendfile`，持久化使用 `mmap`）
- 批量读写 / 压缩





##### 如何保证数据能读成功 / 写成功？

- 生产者：通过 `ISR`（In-Sync Replicas）机制，数据写入主节点和至少 `1` 个 `follower` 后才视为成功，否则重试





##### 什么情况下出现数据丢失？

生产者：首先写数据时有两种方式：同步和异步写

- 同步写时，需要通过 `ack` 参数设置 `n` 个副本都写入成功，才视为写入成功。成功写入的副本叫 `ISR`。如果配置成了 `Leader` 挂掉时允许非 `ISR` 升为 `Leader`，则会丢失数据。

- 异步写时，`Leader` 写入成功就会返回。此情况下 `Leader` 挂掉就会出现数据丢失。

接受者：

- 如果在消息处理完成前就 `commit` 了 `offset`，那么就有可能因为业务失误，造成数据的丢失





##### 为什么不用redis做消息队列？

- 内存资源贵且容量小，存不了太多数据。
- 持久化不够可靠
- `list` 会有 热 `key`、确认机制、不支持多订阅者、二次消费等问题
- `stream` 会把数据存在缓冲区，超过缓冲区后，数据会被丢弃
