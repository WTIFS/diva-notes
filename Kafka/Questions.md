##### Kafka吞吐量大的原因

- 磁盘顺序读写
- 分区分段 + 索引（分`topic`、`broker`、`segment`）
- 零拷贝（发送使用 `sendfile`，持久化使用 `mmap`）
- 批量读写 / 压缩





##### 如何保证数据能读成功 / 写成功？

- 生产者：通过 `ISR`（In-Sync Replicas）机制，数据写入主节点和至少 `1` 个 `follower` 后才视为成功，否则重试





##### 什么情况下出现数据丢失？

- 生产者丢数据
  - 网络问题，没发出去。可以在发送后增加回调，如果回调时发现写入失败可以尝试重写
  - 如果选择异步发送，那么实际发出前消息会存在缓冲区中。此时如果生产者挂了，消息就丢了。

- `broker` 丢数据
  - 首先写数据时有两种方式：同步和异步写
  - 同步写时，需要通过 `ack` 参数设置 `n` 个副本都写入成功，才视为写入成功。成功写入的副本叫 `ISR`。如果配置成了 `Leader` 挂掉时允许非 `ISR` 升为 `Leader`，则会丢失数据。

  - 异步写时，`Leader` 写入成功就会返回。此情况下 `Leader` 挂掉就会出现数据丢失。

- 接收者丢数据
  - 自动提交 `offset`：每个一定的时间间隔，将收到的消息进行 `commit`，和消费消息的过程是异步的。就可能消费失败，但自动 `commit` 了
  - 手动提交 `offset` ：如果在消息处理完成前就提交了 `offset`，那么就有可能因为业务失误，造成数据的丢失





##### 重复消费的情况

手动提交 `offset` 模式下，如果消费完消息之后，还没提交 `offset`，自己就挂掉了，那么这个消息理论上就会被消费两次





##### 怎么保证数据顺序？

写到同一个 `partition` 下





##### 为什么不用redis做消息队列？

- 内存资源贵且容量小，存不了太多数据
- 持久化不够可靠
- `list` 会有 热 `key`、确认机制、不支持多订阅者、二次消费等问题
- `stream` 会把数据存在缓冲区，超过缓冲区后，数据会被丢弃





#### 参考

[Java技术那些事 - Kafka 会不会丢消息？怎么处理的?](https://zhuanlan.zhihu.com/p/307480336)
