#### Kafka吞吐量大的原因

- 磁盘顺序读写
- `page cache` 落盘时，先落缓冲区，再写入硬盘
- 分区分段 + 索引（分`topic`、`broker`、`segment`）
- 零拷贝（落盘使用 `mmap`，读取使用 `sendfile`）
- 批量读写 / 压缩





#### 如何保证数据能读成功 / 写成功？

- 生产者：通过 `ISR`（In-Sync Replicas）机制，数据写入主节点和至少 `1` 个 `follower` 后才视为成功，否则重试





#### 什么情况下出现数据丢失？

- 生产者丢数据
  - 网络问题，没发出去。可以在发送后增加回调，如果回调时发现写入失败可以尝试重写
  - 如果选择异步发送，那么实际发出前消息会存在缓冲区中。此时如果生产者挂了，消息就丢了。
  - 需要通过 `ack` 参数设置 `n` 个副本都写入成功，才视为写入成功。成功写入的副本叫 `ISR`。
    - 如果 `ack` 设的 0，即不管写入结果，则只要服务端写消息时出现任何问题，都会导致消息丢失
    - 如果 `ack` 设的 1，即只写入 `leader` 就视为成功
    - 或者 `ISR` 大小设的 0，或者配置成了 `leader` 挂掉时允许非 `ISR` 升为 `leader`，则 `leader` 挂会导致丢失数据

- `broker` 丢数据
  
  - `kafka` 通过 `Page Cache` 将数据写入磁盘，也是先将数据流写入缓存中，但是什么时候将缓存的数据写入文件中是由操作系统自行决定。
- 接收者丢数据

  - 自动提交 `offset`：每个一定的时间间隔，将收到的消息进行 `commit`，和消费消息的过程是异步的。就可能消费失败，但自动 `commit` 了
  - 手动提交 `offset` ：如果在消息处理完成前就提交了 `offset`，那么就有可能因为业务失误，造成数据的丢失





#### 重复消费的情况

手动提交 `offset` 模式下，如果消费完消息后自己就挂掉了，或者因为网络问题导致无法同步 `offset` 给 `kafka`，那么这个消息就会被消费。

如果消息处理时间过长，会导致消费者被判断为宕机，进而被踢出消费者组，触发 `rebanlance`。于是别的机器可能重复消费到消息。





#### 怎么保证数据顺序？

写到同一个 `partition` 下





#### 为什么不用redis做消息队列？

- 内存资源贵且容量小，存不了太多数据
- 持久化不够可靠
- `list` 会有 热 `key`、确认机制、不支持多订阅者、二次消费等问题
- `stream` 会把数据存在缓冲区，超过缓冲区后，数据会被丢弃






#### 生产者发送消息有哪些模式

- 发后即忘 fire-and-forget
  - 只管往 `kafka` 里面发消息，但是**不关心消息是否正确到达**。效率最高，但是可靠性也最差

- 同步 sync

- 异步 async
  - `producer.send()` 传入一个回调函数，消息不管成功或者失败都会调用这个回调函数，这样就是异步发送
  - 在回调函数中选择记录日志还是重试都取决于调用方






#### Kafka 支持读写分离吗

Kafka 是**不支持读写分离**的。那么读写分离的好处是什么？主要就是让一个节点去承担另一个节点的负载压力，也就是能做到一定程度的负载均衡，而 kafka 不通过读写分离也可以一定程度上去实现负载均衡。而且对于 Kafka 的架构来说，读写分离有两个很大的缺点：数据不一致和时延问题

1. 数据不一致的问题：读写分离必然涉及到数据的同步，只要是**不同节点之间的数据同步**，必然**会有数据不一致的问题**存在。

2. 延时问题：由于 Kafka 独特的数据处理方式，导致如果将数据从一个节点同步到另一个节点必然会经过**主节点磁盘和从节点磁盘**，对一些延时性要求较高的应用来说，并不太适用





#### 参考

[Java技术那些事 - Kafka 会不会丢消息？怎么处理的?](https://zhuanlan.zhihu.com/p/307480336)
